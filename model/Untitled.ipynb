{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6b61b1-9f58-45aa-858c-4daaf34ad772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from data import TrainDataset, ValDataset, display_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from models import Generator, Discriminator\n",
    "from loss import GeneratorLoss\n",
    "from metric import ssim\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "## Set the seed for reproducibility\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf20de99-23cf-43cd-8009-6e9303940558",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e731f-9b72-4cb6-bbea-e39e462966a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(upscale_factor=4).to(DEVICE)\n",
    "print(\"# generator parameters:\", sum(param.numel() for param in netG.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126a0e6-80a2-45b2-94ab-6b6aa12de425",
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = Discriminator().to(DEVICE)\n",
    "print(\"# discriminator parameters:\", sum(param.numel() for param in netD.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c535abf-207d-4f84-b0b7-7d49f7be0f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_criterion = GeneratorLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61e68b-66a6-4d9d-9e8a-45594030e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the optimizer\n",
    "optimizerG = torch.optim.AdamW(netG.parameters(), lr=1e-3)\n",
    "optimizerD = torch.optim.AdamW(netD.parameters(), lr=1e-3)\n",
    "\n",
    "## Initialize the dictionary to store the results\n",
    "results = {\n",
    "    \"d_loss\": [],\n",
    "    \"g_loss\": [],\n",
    "    \"d_score\": [],\n",
    "    \"g_score\": [],\n",
    "    \"psnr\": [],\n",
    "    \"ssim\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7cf190-b557-4cf8-bef8-e0839ed15581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize parameters\n",
    "CROP_SIZE = 100\n",
    "UPSCALE_FACTOR = 4\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## Load the train dataset\n",
    "print(\"[INFO] Loading Train dataset\")\n",
    "train_set = TrainDataset(\"../dataset/train\", crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "\n",
    "## Load the validation dataset\n",
    "print(\"[INFO] Loading Val dataset\")\n",
    "val_set = ValDataset(\"../dataset/valid\", crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4ac0f-d152-44da-86b5-a6ea7a9e9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the train data loader\n",
    "print(\"[INFO] Creating Train data loader\")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "## Create the validation data loader\n",
    "print(\"[INFO] Creating Val data loader\")\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79c586-390e-49b0-a28a-124547cc113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start training\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "\n",
    "    ## Progress bar for training\n",
    "    train_bar = tqdm(train_loader, total=len(train_loader))\n",
    "\n",
    "    ## Initialize the dictionary to store the results\n",
    "    running_results = {\"batch_sizes\": 0, \n",
    "                       \"d_loss\": 0, \"g_loss\": 0,\n",
    "                       \"d_score\": 0, \"g_score\": 0,\n",
    "                    }\n",
    "\n",
    "    ## Set the models to train mode\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "\n",
    "    ## Iterate over the batch of images\n",
    "    for lr_img, hr_img in train_bar:\n",
    "\n",
    "        ## Get the current batch size\n",
    "        batch_size = lr_img.size(0)\n",
    "        running_results[\"batch_sizes\"] += batch_size\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize D(x)-1-D(G(z))\n",
    "        ###########################\n",
    "\n",
    "        ## Move the images to the GPU\n",
    "        hr_img = hr_img.to(DEVICE) # high resolution image\n",
    "        lr_img = lr_img.to(DEVICE) # low resolution image\n",
    "        with torch.no_grad():\n",
    "            sr_img = netG(lr_img) # super resolution image\n",
    "\n",
    "        ## Set the gradients of Discriminator to zero\n",
    "        netD.zero_grad()\n",
    "\n",
    "        ## Formward propagate the HR image and SR image through the discriminator\n",
    "        real_out = netD(hr_img).mean()\n",
    "        fake_out = netD(sr_img).mean()\n",
    "\n",
    "        ## Calculate the discriminator loss\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "\n",
    "        ## Backpropagate the loss\n",
    "        d_loss.backward(retain_graph=True)\n",
    "\n",
    "        ## Update the weights\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "        ###########################\n",
    "        \n",
    "        ## Forward propagate the SR image through the discriminator\n",
    "        with torch.no_grad():\n",
    "            fake_out = netD(sr_img).mean()\n",
    "        \n",
    "        ## Set the gradients of the Generator to zero\n",
    "        netG.zero_grad()\n",
    "\n",
    "        ## Forward propagate the LR image through the generator to get the SR image\n",
    "        sr_img = netG(lr_img)\n",
    "\n",
    "        ## Calculate the generator loss\n",
    "        g_loss = generator_criterion(fake_out, sr_img, hr_img)\n",
    "\n",
    "        ## Backpropagate the loss\n",
    "        g_loss.backward()\n",
    "\n",
    "        ## Update the weights\n",
    "        optimizerG.step()\n",
    "\n",
    "        ## Store loss for current batch\n",
    "        running_results[\"g_loss\"] += g_loss.item() * batch_size\n",
    "        running_results[\"d_loss\"] += d_loss.item() * batch_size\n",
    "        running_results[\"d_score\"] += real_out.item() * batch_size\n",
    "        running_results[\"g_score\"] += fake_out.item() * batch_size\n",
    "\n",
    "        ## Update the progress bar and print the results\n",
    "        train_bar.set_description(\n",
    "            desc=\"[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f\"\n",
    "            % (\n",
    "                epoch,\n",
    "                NUM_EPOCHS,\n",
    "                running_results[\"d_loss\"] / running_results[\"batch_sizes\"],\n",
    "                running_results[\"g_loss\"] / running_results[\"batch_sizes\"],\n",
    "                running_results[\"d_score\"] / running_results[\"batch_sizes\"],\n",
    "                running_results[\"g_score\"] / running_results[\"batch_sizes\"],\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ## Set the Generator to evaluation mode\n",
    "    netG.eval()\n",
    "\n",
    "    ## Run the validation loop\n",
    "    with torch.no_grad():\n",
    "\n",
    "        ## Progress bar for validation loop\n",
    "        val_bar = tqdm(val_loader, total=len(val_loader))\n",
    "\n",
    "        ## Initialize the dictionary to store the results\n",
    "        valing_results = {\n",
    "            \"mse\": 0,\n",
    "            \"ssims\": 0,\n",
    "            \"psnr\": 0,\n",
    "            \"ssim\": 0,\n",
    "            \"batch_sizes\": 0,\n",
    "        }\n",
    "\n",
    "        val_images = []\n",
    "\n",
    "        ## Iterate over the batch of images\n",
    "        for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "\n",
    "            ## Get the current batch size\n",
    "            batch_size = val_lr.size(0)\n",
    "            valing_results[\"batch_sizes\"] += batch_size\n",
    "\n",
    "            ## Move the images to the GPU\n",
    "            lr = val_lr\n",
    "            hr = val_hr\n",
    "            if torch.cuda.is_available():\n",
    "                lr = lr.cuda()\n",
    "                hr = hr.cuda()\n",
    "\n",
    "            ## Forward propagate the LR image through the generator to get the SR image\n",
    "            sr = netG(lr)\n",
    "\n",
    "            ## Calculate All the metrics\n",
    "            ## Calculate and store the MSE\n",
    "            batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "            valing_results[\"mse\"] += batch_mse * batch_size\n",
    "\n",
    "            ## Calculate and store the SSIMs\n",
    "            batch_ssim = ssim(sr, hr).item()\n",
    "            valing_results[\"ssims\"] += batch_ssim * batch_size\n",
    "\n",
    "            ## Calculate and store the PSNR\n",
    "            valing_results[\"psnr\"] = 10 * math.log10( (hr.max() ** 2) / (valing_results[\"mse\"] / valing_results[\"batch_sizes\"]))\n",
    "\n",
    "            ## Calculate and store the SSIM\n",
    "            valing_results[\"ssim\"] = (valing_results[\"ssims\"] / valing_results[\"batch_sizes\"])\n",
    "\n",
    "            ## Update the progress bar and print the results\n",
    "            val_bar.set_description(\n",
    "                desc=\"[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f\"\n",
    "                % (valing_results[\"psnr\"], valing_results[\"ssim\"])\n",
    "            )\n",
    "\n",
    "            ## Store the HR, SR and Val_hr_restore images\n",
    "            val_images.extend(\n",
    "                [\n",
    "                    display_transform(val_hr_restore.squeeze(0)),\n",
    "                    display_transform(hr.data.cpu().squeeze(0)),\n",
    "                    display_transform(sr.data.cpu().squeeze(0)),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        ## Store the validation results for the current epoch\n",
    "        val_images = torch.stack(val_images)\n",
    "        val_images = torch.chunk(val_images, val_images.size(0) // 15)\n",
    "        val_save_bar = tqdm(val_images, desc=\"[saving training results]\")\n",
    "\n",
    "        ## Save the images to the disk\n",
    "        index = 1\n",
    "        out_path = \"./results/\"\n",
    "        for image in val_save_bar:\n",
    "            image = torchvision.utils.make_grid(image, nrow=3, padding=5)\n",
    "            torchvision.utils.save_image(\n",
    "                image,\n",
    "                out_path + \"epoch_%d_index_%d.png\" % (epoch, index),\n",
    "                padding=5,\n",
    "            )\n",
    "            index += 1\n",
    "\n",
    "    ## save model parameters every epoch\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "\n",
    "    ## Save the Generator model\n",
    "    torch.save({\"model\": netG.state_dict()},\n",
    "        f\"./checkpoints/netG_{UPSCALE_FACTOR}x_epoch{epoch}.pth.tar\")\n",
    "\n",
    "    ## Save the Discriminator model\n",
    "    torch.save({\"model\": netD.state_dict()},\n",
    "        f\"./checkpoints/netD_{UPSCALE_FACTOR}x_epoch{epoch}.pth.tar\")\n",
    "\n",
    "    ## Store the losses and scores for the current epoch\n",
    "    results[\"d_loss\"].append(running_results[\"d_loss\"] / running_results[\"batch_sizes\"])\n",
    "    results[\"g_loss\"].append(running_results[\"g_loss\"] / running_results[\"batch_sizes\"])\n",
    "    results[\"d_score\"].append(running_results[\"d_score\"] / running_results[\"batch_sizes\"])\n",
    "    results[\"g_score\"].append(running_results[\"g_score\"] / running_results[\"batch_sizes\"])\n",
    "    results[\"psnr\"].append(valing_results[\"psnr\"])\n",
    "    results[\"ssim\"].append(valing_results[\"ssim\"])\n",
    "\n",
    "    ## Save the results every 10 epochs\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        out_path = \"./logs/\"\n",
    "        data_frame = pd.DataFrame(\n",
    "            data={\n",
    "                \"Loss_D\": results[\"d_loss\"],\n",
    "                \"Loss_G\": results[\"g_loss\"],\n",
    "                \"Score_D\": results[\"d_score\"],\n",
    "                \"Score_G\": results[\"g_score\"],\n",
    "                \"PSNR\": results[\"psnr\"],\n",
    "                \"SSIM\": results[\"ssim\"],\n",
    "            },\n",
    "            index=range(1, epoch + 1),\n",
    "        )\n",
    "\n",
    "        ## Save the results to the disk\n",
    "        data_frame.to_csv(\n",
    "            out_path + \"ssrgan_\" + str(UPSCALE_FACTOR) + \"_train_results.csv\",\n",
    "            index_label=\"Epoch\",\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a70a7-d259-4e52-9b31-c9e1f6c8000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_memory_bytes = lr.element_size() * lr.nelement()\n",
    "print(f\"Total memory usage of the tensor: {total_memory_bytes/1e+9} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e456a5-b7e8-4b77-b078-e1f59134521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e39ab9-9d6d-447d-8850-386053b1e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(hr_img)/1e+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac3900-872e-4212-adb0-ac95938cee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_img.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c344f0-4312-41c7-9a09-dcff5232f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ed4d1-6c0f-4356-91b6-d03e327827b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bcb32-d1e2-47de-b073-d81cc5a828d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea8a17c2-8c5e-4ea5-8499-5eade672755b",
   "metadata": {},
   "source": [
    "## Inferencemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6697c415-5c76-4f65-ab20-b1f0fe3b52f9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (initial): ConvBlock(\n",
       "    (cnn): SeperableConv2d(\n",
       "      (depthwise): Conv2d(3, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=3)\n",
       "      (pointwise): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (bn): Identity()\n",
       "    (act): PReLU(num_parameters=64)\n",
       "  )\n",
       "  (residual): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (5): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (6): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (7): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (8): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (9): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (10): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (11): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (12): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (13): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (14): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "    (15): ResidualBlock(\n",
       "      (block1): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "      (block2): ConvBlock(\n",
       "        (cnn): SeperableConv2d(\n",
       "          (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "          (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): PReLU(num_parameters=64)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (convblock): ConvBlock(\n",
       "    (cnn): SeperableConv2d(\n",
       "      (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "      (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): PReLU(num_parameters=64)\n",
       "  )\n",
       "  (upsampler): Sequential(\n",
       "    (0): UpsampleBlock(\n",
       "      (conv): SeperableConv2d(\n",
       "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "        (pointwise): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (ps): PixelShuffle(upscale_factor=2)\n",
       "      (act): PReLU(num_parameters=64)\n",
       "    )\n",
       "    (1): UpsampleBlock(\n",
       "      (conv): SeperableConv2d(\n",
       "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "        (pointwise): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (ps): PixelShuffle(upscale_factor=2)\n",
       "      (act): PReLU(num_parameters=64)\n",
       "    )\n",
       "  )\n",
       "  (final_conv): SeperableConv2d(\n",
       "    (depthwise): Conv2d(64, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=64)\n",
       "    (pointwise): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "model = Generator(upscale_factor=4).to(DEVICE)\n",
    "state_dict = torch.load('../checkpoints/netG_4x_epoch15.pth.tar')  # Load model weights\n",
    "model.load_state_dict(state_dict[\"model\"])\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d71c295-26db-461a-9f8d-d42a0db79d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec707d24-d896-4874-bb6f-b12122b173c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "hr_image = Image.open('../dataset/valid/00001273_000.png').convert('RGB')\n",
    "\n",
    "\n",
    "## Create the LR image transformer by downsampling the HR image and applying bicubic interpolation\n",
    "lr_scale = transforms.Resize((256,256), interpolation=Image.BICUBIC)\n",
    "\n",
    "## Create the restored HR image tranformer (simple classical method) by upsampling the LR image and applying bicubic interpolation\n",
    "hr_scale = transforms.Resize((1024,1024), interpolation=Image.BICUBIC)\n",
    "\n",
    "## Create the LR Image from the original HR Image using the LR Image transformer\n",
    "lr_image = lr_scale(hr_image)\n",
    "\n",
    "## Create the restored HR Image from the LR Image using the classical method of restored HR Image transforms\n",
    "hr_restore_img = hr_scale(lr_image)\n",
    "\n",
    "# Move the image and model to GPU if available\n",
    "lr_image = to_tensor(lr_image).cuda()\n",
    "lr_image = lr_image.unsqueeze(0)\n",
    "\n",
    "lr_image.shape\n",
    "\n",
    "# Perform model inference\n",
    "with torch.no_grad():\n",
    "    output = model(lr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee93a17d-fce7-4a7e-9624-732cf138fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = output.squeeze(0)\n",
    "\n",
    "## Transforms for displaying the images\n",
    "display_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "out = display_transform(out)\n",
    "\n",
    "out.save(\"output.png\")\n",
    "hr_image.save(\"input.png\")\n",
    "hr_restore_img.save(\"naive.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2d00a-30fd-422f-a368-97c83219636f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d1651-f843-46e7-9c92-cf42a36e9a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c4e98-6985-49ae-ae3f-11e81111aec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1abdb1-9cf9-44eb-bbb2-09c70cc0e7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipi_ind_project",
   "language": "python",
   "name": "aipi_ind_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
